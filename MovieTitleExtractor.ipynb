{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Extracting Movie Titles with Hidden Markov Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "English sentences follow a similar structure when it comes to referring to a named entity.  There is a lot of success in named entity recognition when extracting locations, times, and people from text.  Less successful, however, is when we are dealing with names that are synonymous with common words.  Even moreso when the data we are dealing with informal data, such as tweets from Twitter.  This experiment aims to extract movie titles from Twitter data using Hidden Markov Models (HMMs).  The main idea is that while every tweet is unique, a common sentence structure (part-of-speech tags) sequence can be learned from all of the tweets.\n",
    "\n",
    "We will be using HMMs to build a language model from our training data.  Our training data consists of tweets that mention a movie, and include other tweets that do not mention a movie, but may contain a word that is synonymous with a movie.  For example,\n",
    "\n",
    "* \" did you do your homework yet ? \" \" no I 'm watching MovieTitle ( Frozen ) \"\n",
    "* So my feet are frozen .... I leave the windows open , 24/7\n",
    "* Genuinely love MovieTitle ( frozen ) & lt ; 3\n",
    "* MovieTitle ( FROZEN ) IN 3 D IS THE BEST THING EVER\n",
    "\n",
    "After creating our language model, we will test it using two separate datasets, each with their own gazetteers.  The first test dataset contains movies that are the same as those in our training dataset, while the second dataset contains movies that are not the same as those in our training dataset.  Both datasets may contain tweets that do not refer a movie, but still contain a word that is synonymous with a movie.\n",
    "\n",
    "Small sample of tweets from test dataset 1:\n",
    "* my face is pretty much frozen\n",
    "* Finally getting to watch MovieTitle ( Frozen ) with Kynnedi ❄️\n",
    "* About to go watch MovieTitle ( Frozen ) .\n",
    "* the kids im babysitting are listening to music from MovieTitle ( frozen ) #ok\n",
    "\n",
    "Small sample of tweets from test dataset 2:\n",
    "* Ok .... Matthew did the damn thing in MovieTitle ( Dallas Buyers Club )\n",
    "* This is funnier than her crying at MovieTitle ( the LEGO movie )\n",
    "* RT @birbigs : MovieTitle ( Dallas Buyers Club ) is one of the rare movies that lives up to the hype . If you have n't already , SEE IT .\n",
    "* RT @loadedmag RT 2 Win ! In cinemas today , MovieTitle ( Lone survivor ) , four men face an army , To celebrate were giving away KINDLE FIRE ! #LOSUUK\n",
    "\n",
    "\n",
    "Training and test data was obtained from: https://github.com/sandeepAshwini/TwitterMovieData\n",
    "\n",
    "Movie titles can be downloaded from http://www.imdb.com/interfaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, let's get these import statements out of the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use nltk for part-of-speech tagging, and defaultdict for maintaining a dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, we notice that there is a lot of noise in each tweet that does not contribute to the reference of a movie.  For example, hyperlinks, double quotation marks, and \"RT\" does not help decipher whether a user is referring to a movie or not.  Because of this, we need to normalize the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def normalize(line):\n",
    "    tokenized_line = line.split()\n",
    "    new_list = []\n",
    "    for token in tokenized_line:\n",
    "        if not any(x in token for x in [\"@\", \"\\\"\", \"http://\", \"RT\"]):\n",
    "            new_list.append(token)\n",
    "        if \"@\" in token:\n",
    "            new_list.append(\"Cody\")\n",
    "    return \" \".join(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the above code describes, we go through the entire line, removing anything that's a mention/reply, double quote, hyperlink, and RT.  We also make sure to be very careful about mentions.  Instead of removing them, we note that they actually contain useful information regarding the context of the tweet, so we substitute it for an actual proper noun instead.  I substituted it for my name, but I'm sure a unique placeholder would be a better idea.\n",
    "\n",
    "Next, in order to build a language model from our training data, we need to replace the substring \"..MovieTitle ( * )..\" in the tweet with a placeholder string.  In this case, we use the substring \"#MOVIE#\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def switch_title_with_placeholder(line):\n",
    "    before_title = line[:line.find(\"MovieTitle (\") - 1]\n",
    "    including_title = line[line.find(\"MovieTitle (\"):]\n",
    "    after_title = including_title[including_title.find(\")\") + 2:]\n",
    "    if line.find(\"MovieTitle (\") == 0:\n",
    "        return \" \".join([\"#MOVIE#\", after_title])\n",
    "    else:\n",
    "        return \" \".join([before_title, \"#MOVIE#\", after_title])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to train our language model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a Hidden Markov Model uses initial states, transitions, and emissions from states, we need to define these as well as find their probabilities.  We will let the states be defined as the word's part-of-speech tag, and the emission will be defined as the word itself.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_and_store_pos_tag(line):\n",
    "    splitted_line = line.split()\n",
    "    pos_tags = nltk.pos_tag(splitted_line)\n",
    "    for item in pos_tags:\n",
    "        if item[0] == \"#MOVIE#\":\n",
    "            current_word = \"#MOVIE#\"\n",
    "            current_tag = \"#MOVIE#\"\n",
    "        else:\n",
    "            current_word = item[0]\n",
    "            current_tag = item[1]\n",
    "        if pos_tags.index(item) == 0:\n",
    "            initial_state_counts[current_tag] += 1\n",
    "        elif pos_tags.index(item) >= 1:\n",
    "            if state_transition_counts[previous_tag] == False:\n",
    "                state_transition_counts[previous_tag] = defaultdict(lambda: 0.001)\n",
    "            state_transition_counts[previous_tag][current_tag] += 1\n",
    "            dest_src_transitions[current_tag].append(previous_tag)\n",
    "        if state_output_counts[current_tag] == False:\n",
    "            state_output_counts[current_tag] = defaultdict(lambda: 0.001)\n",
    "        state_output_counts[current_tag][current_word] += 1\n",
    "        outputs_to_states[current_word].append(current_tag)\n",
    "        previous_tag = current_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function above determines a tweet's part-of-speech tags and stores the number of times:\n",
    "* That state (ie. part-of-speech tag) is at the beginning of the tweet\n",
    "* We transition from one state to another\n",
    "* The word is emitted from that state\n",
    "\n",
    "Some states may not transition from one state to another in our training data yet arise in our test data, so we assign those occurences with a probability of 0.001.  We do the same thing for our emission probabilities (ie. if we come across an emission that we have never seen in our training data, we assign it a small probability of 0.001).\n",
    "\n",
    "Also note the variables dest_src_transitions and outputs_to_states.  These variables make it easier and quicker to do a lookup of the states and outputs later on when we need to calculate the probability of a certain sequence.\n",
    "* dest_src_transitions is a dictionary where key=destination state, and value=list of states that transition to that destination state\n",
    "* outputs_to_states is a dictionary where key=emitted word, and value=list of states that emit that word.\n",
    "\n",
    "Next, we want to convert those counts to actual probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def convert_counts_to_probs():\n",
    "    total_initial_states = 0\n",
    "    for item in initial_state_counts.iterkeys():\n",
    "        total_initial_states += initial_state_counts[item]\n",
    "    for item in initial_state_counts.iterkeys():\n",
    "        initial_state_counts[item] = float(initial_state_counts[item])/float(total_initial_states)\n",
    "\n",
    "    for start_key in state_transition_counts.iterkeys():\n",
    "        total_transition_counts = 0\n",
    "        for end_key in state_transition_counts[start_key]:\n",
    "            total_transition_counts += state_transition_counts[start_key][end_key]\n",
    "        for end_key in state_transition_counts[start_key]:\n",
    "            state_transition_counts[start_key][end_key] = float(state_transition_counts[start_key][end_key]) / float(\n",
    "                total_transition_counts)\n",
    "\n",
    "    for state in state_output_counts.iterkeys():\n",
    "        total_output_counts = 0\n",
    "        for output in state_output_counts[state]:\n",
    "            total_output_counts += state_output_counts[state][output]\n",
    "        for output in state_output_counts[state]:\n",
    "            state_output_counts[state][output] = float(state_output_counts[state][output]) / float(\n",
    "                total_output_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now pipe the functions above together to create our language model from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train(train_data):\n",
    "    with open(train_data) as train_file:\n",
    "        for line in train_file.readlines():\n",
    "            normalized_line = normalize(line)\n",
    "            while \"MovieTitle (\" in normalized_line:\n",
    "                normalized_line = switch_title_with_placeholder(normalized_line)\n",
    "            get_and_store_pos_tag(normalized_line)\n",
    "        convert_counts_to_probs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function trains our language model over all tweets in our training data.  Some tweets refer to more than one movie, so we iterate over the tweet as many times as necessary in order to replace all movie titles with placeholder text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Forward Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our language model, we will use the Forward algorithm.  This algorithm uses dynamic programming (stores results in a table in order to avoid recalculations) in order to calculate the probability of a sequence of emissions.  Given table v, state q, and index t, the forward algorithm is as follows:\n",
    "~~~\n",
    "for each state q that emits the word at index 1\n",
    "    v[q, 1] = Pr(initial state=q) * Pr(state=q, emission=word at index 1)\n",
    "from t = 2 to end of tweet\n",
    "    v[q, t] += v[q', t-1] * Pr(src state=q', dest state=q) * Pr(current state=q, emission=word at index t)\n",
    "~~~\n",
    "Our implementation will follow the same algorithm, but will also incorporate some smoothing in areas where the Forward Algorithm is too harsh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def forward_algorithm(line):\n",
    "    splitted_line = line.split()\n",
    "    v = defaultdict(float)\n",
    "    probability = 0.001\n",
    "    for item in set(outputs_to_states[splitted_line[0]]):\n",
    "        v[(item, 0)] = initial_state_counts[item] * state_output_counts[item][splitted_line[0]]\n",
    "    for x in range(1, len(splitted_line)):\n",
    "        for item_dest in set(outputs_to_states[splitted_line[x]]):\n",
    "            for item in set(outputs_to_states[splitted_line[x-1]]):\n",
    "                if v[(item, x-1)] == 0.0:\n",
    "                    v[(item, x - 1)] = probability * 0.0000001\n",
    "                v[(item_dest, x)] += (v[(item, x-1)] * state_transition_counts[item][item_dest] * state_output_counts[item_dest][splitted_line[x]])\n",
    "                probability = v[(item_dest, x)]\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems in the original Forward algorithm occur when there is no state in that emits a word.  Since there is no state that emits it, our outputs_to_states value for that emission key would be empty, thus causing us to move onto the next iteration of the loop.  This puts 0 in those table slots, which ruins our algorithm.  Our workaround is the following: If there is no state that emits a specific state that we are looking up, assign it as the old probability * a very very low probability.  This causes our algorithm to favor tweets that contain words and part-of-speech tags that we are more familiar with.\n",
    "\n",
    "We are now ready to evaluate our test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have defined the Forward algorithm, we can now attempt to extract movie titles from them.  To do this we need to normalize the text, extract the original tweet from the \"..MovieTitle ( * )..\" tagged tweet, generate candidate movies by comparing the words in the sentence with the movies in our gazetteer, and then perform the Forward algorithm on each candidate.  Candidates that score higher than the original tweet are considered as movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_original_sentence(sentence_number, line):\n",
    "    before_title = line[:line.find(\"MovieTitle (\") - 1]\n",
    "    including_title = line[line.find(\"MovieTitle (\"):]\n",
    "    movie_title = including_title[len(\"MovieTitle (\"):including_title.find(\")\")]\n",
    "    after_title = including_title[including_title.find(\")\") + 2:]\n",
    "    gazetteer.add(movie_title[1:-1])\n",
    "    sentence_to_movies[sentence_number].append(movie_title[1:-1])\n",
    "    if line.find(\"MovieTitle (\") == 0:\n",
    "        return movie_title[1:] + after_title\n",
    "    else:\n",
    "        return before_title + movie_title + after_title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is very similar to `switch_title_with_placeholder(line)`, the difference being that it removes the MovieTitle tags, and the movie to a gazetteer for searching through later.\n",
    "\n",
    "We can now pipe our functons together to evaluate our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test(test_data):\n",
    "    global true_positive\n",
    "    global false_negative\n",
    "    global false_positive\n",
    "    with open(test_data) as test_file:\n",
    "        original_sentences = []\n",
    "        for (i, line) in enumerate(test_file.readlines()):\n",
    "            normalized_line = normalize(line)\n",
    "            while \"MovieTitle (\" in normalized_line:\n",
    "                normalized_line = get_original_sentence(i, normalized_line)\n",
    "            original_sentences.append(normalized_line)\n",
    "        for sentence in original_sentences:\n",
    "            potential_movies = []\n",
    "            print sentence\n",
    "            original_prob = forward_algorithm(sentence)\n",
    "            candidate_indices = []\n",
    "            splitted_sentence = sentence.split()\n",
    "            for x in range(0, len(splitted_sentence)):\n",
    "                for y in range(x+1, len(splitted_sentence)+1):\n",
    "                    if \" \".join(splitted_sentence[x:y]) in gazetteer:\n",
    "                        candidate_indices.append([x, y])\n",
    "            for candidate_indice in candidate_indices:\n",
    "                splitted_sentence = sentence.split()\n",
    "                movie_title = \" \".join(splitted_sentence[candidate_indice[0]:candidate_indice[1]])\n",
    "                splitted_sentence[candidate_indice[1] - 1] = \"#MOVIE#\"\n",
    "                del splitted_sentence[candidate_indice[0]:candidate_indice[1] - 1]\n",
    "                candidate_sentence = \" \".join(splitted_sentence)\n",
    "                candidate_prob = forward_algorithm(candidate_sentence)\n",
    "                if candidate_prob > original_prob:\n",
    "                    potential_movies.append(movie_title)\n",
    "\n",
    "            actual = set(sentence_to_movies[original_sentences.index(sentence)])\n",
    "            candidates = set(potential_movies)\n",
    "            print \"Actual:\", \", \".join(actual)\n",
    "            print \"Guesses:\", \", \".join(candidates)\n",
    "            local_true_positive = len(actual.intersection(candidates))\n",
    "            if len(actual) == 0 and len(candidates) == 0:\n",
    "                true_positive += 1\n",
    "            local_false_negative = len(actual - candidates)\n",
    "            local_false_positive = len(candidates - actual)\n",
    "\n",
    "            true_positive += local_true_positive\n",
    "            false_negative += local_false_negative\n",
    "            false_positive += local_false_positive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables at the end of the function (ie. true positive, false negative, false positive) are used for quantifying the quality of our algorithm.  True positive refers to movie titles that our algorithm thinks is correct and is correct (based on the tagged movies in that tweet).  False negative refers to movie titles that our algorithm dismissed but is correct.  False positive refers to movie titles that our algorithm thinks is correct but is not correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_state_counts = defaultdict(float)\n",
    "state_transition_counts = defaultdict(bool)\n",
    "state_output_counts = defaultdict(bool)\n",
    "outputs_to_states = defaultdict(list)\n",
    "dest_src_transitions = defaultdict(list)\n",
    "\n",
    "gazetteer = set()\n",
    "with open(\"movies.list\") as movies_list:\n",
    "    for index, line in enumerate(movies_list):\n",
    "        if index >= 16 and line[0] != \"\\\"\":\n",
    "            gazetteer.add(line.split(\"(\")[0])\n",
    "\n",
    "true_positive = 0\n",
    "false_negative = 0\n",
    "false_positive = 0\n",
    "\n",
    "train('TrainFile.labeled')\n",
    "sentence_to_movies = defaultdict(list)\n",
    "test('TestFile1.labeled')\n",
    "\n",
    "precision = float(true_positive)/float(true_positive + false_positive)\n",
    "recall = float(true_positive)/float(true_positive + false_negative)\n",
    "\n",
    "print \"True Positive =\", true_positive\n",
    "print \"False Negative =\", false_negative\n",
    "print \"False Positive =\", false_positive\n",
    "print \"Precision =\", precision\n",
    "print \"Recall =\", recall\n",
    "print \"F-measure =\", float(2 * precision * recall)/float(precision + recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`movies.list` was downloaded from the IMDB interfaces.  It contains a list of movies that we can use to compare with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of Test Dataset 1\n",
    "\n",
    "Condition | Result\n",
    "-------------|-------------\n",
    "True Positive | 138\n",
    "False Negative | 9\n",
    "False Positive | 67\n",
    "Precision | 0.673170731707\n",
    "Recall | 0.938775510204\n",
    "F-measure | 0.784090909091\n",
    "\n",
    "### Results of Test Dataset 2\n",
    "\n",
    "Condition | Result\n",
    "-------------|-------------\n",
    "True Positive | 128\n",
    "False Negative | 36\n",
    "False Positive | 22\n",
    "Precision | 0.853333333333\n",
    "Recall | 0.780487804878\n",
    "F-measure | 0.815286624204\n",
    "\n",
    "\n",
    "But what do these conditions mean?  Precision tells us the ratio of correctly guessed movies to correctly guessed movies and incorrectly guessed movies.  Recall tells us the ratio of correctly guessed movies to correctly guessed movies and incorrectly ignored movies.  F-measure is calculated by combining the precision and recall.  A high recall and low precision would yield the same F-measure as a high precision and a low recall would.\n",
    "\n",
    "This means that in test dataset 1 our algorithm thinks less correct movies are incorrect, but thinks too many incorrect movies are correct.  The result is different for test dataset 2: it thinks less incorrect movies are correct, but thinks too many correct movies are incorrect.\n",
    "\n",
    "In this scenario, it's difficult to know which of higher precision or higher recall is better.  We can obtain a higher recall score by accepting all candidate movies in our `test(test_data)` function.  This means that all candidate movies will be returned as \"guessed\" movies which brings our recall score much higher, but at the same time it means that it will return *every* movie that it sees, as long as the movie is in the gazetteer.  This isn't very wise.\n",
    "\n",
    "With that said, it does a pretty good job of actually extracting movies from the tweets as well as ignoring words that are synonymous with movie titles.  For example,\n",
    "\n",
    "* **Normalized tweet:** #OhHeyGuys would you rather I tweet #AmericanHustle or #Hobbit tomorrow ?\n",
    "* **Actual movies:** #Hobbit, #AmericanHustle\n",
    "* **Guessed movies:** #Hobbit, #AmericanHustle\n",
    "\n",
    "\n",
    "* **Normalized tweet:** Cody seeing hobbit tonight ! Very excited . #bilbo #Hobbit\n",
    "* **Actual:** hobbit\n",
    "* **Guesses:** hobbit\n",
    "\n",
    "\n",
    "* **Normalized tweet:** Me and a giant #Hobbit #dwarf #oxymoron at Auckland International Airport . See you shortly ...\n",
    "* **Actual movie**: \n",
    "* **Guessed movie:**\n",
    "\n",
    "The first example shows that our algorithm correctly extracts two movie titles from the tweet.  The second example shows that our algorithm is picks out the correct movie reference (\"hobbit\" and \"#Hobbit\" are in the gazetteer).  The third example shows that it doesn't consider #Hobbit to be a movie title.  While it is a movie title, the tweet refers to #Hobbit as a character, not the movie.\n",
    "\n",
    "The biggest drawback to our algorithm is that it only finds movies that are in the gazetteer (ie. list of movie titles).  This means that without the gazetteer, it will not find any movies.  This is a problem because we would need to keep an updated list of movies and their varying reference phrases (e.g. Lord of the Rings, LOTR, LotR).  For example,\n",
    "\n",
    "* **Normalized tweet:** The Lego movie was hilarious ! Sweet too ! And Nandos was gorgeous ! #twopuddings Cody ;)\n",
    "* **Actual movie:** The Lego movie\n",
    "* **Guessed movies:** Lego movie, The Lego movie\n",
    "\n",
    "\"Lego movie\" and \"The Lego movie\" refer to the same movie, yet the program separates them because the two are separate entries in the gazetteer.  This is not optimal.  A post-processing step such as a gazetteer entry lookup function would solve this, but as stated before, requires an up-to-date list of variations of movie titles."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
